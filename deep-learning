# Deep dive into deep learning

## Machine learning basics

Jensen-Shannon divergence is derived from Kullback-Leibler divergence; both essentially aim to capture the similiarity between two different probabiltiy distributions.

Objective: understand the implications of KL divergence, how JS is derived from KL, how all of this relates to Kolmogorov-Smirnov
